{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim:\n",
    "To create a resume-checking tool that extracts, preprocesses, and standardizes text from resumes in multiple formats (.docx, .pdf, .txt), enabling the identification of key information such as skills, experience, and qualifications for recruitment purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:\n",
    "The project aims to automate the extraction and normalization of information from resumes to enable key insights, possibly for skills identification, experience analysis, or matching candidates to job roles based on text features in resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "import docx\n",
    "import PyPDF2\n",
    "import zipfile\n",
    "import re\n",
    "from docx import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\nidhi\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\nidhi\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\nidhi\\appdata\\roaming\\python\\python311\\site-packages (from python-docx) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\nidhi\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nidhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nidhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nidhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Nidhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download essential NLTK resources for text preprocessing\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def to_lowercase(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "#Removes common English words that don't add much meaning (like 'the', 'is').\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    return [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "#Reduces words to their root form to improve matching (e.g., 'running' to 'run').\n",
    "def apply_lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applies all preprocessing steps to text:tokenizing, converting to lowercase, removing stopwords/punctuation, and lemmatizing.\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = simple_tokenize(text)\n",
    "    tokens = to_lowercase(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = remove_punctuation(tokens)\n",
    "    tokens = apply_lemmatization(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(file_path):\n",
    "    \"\"\"Reads and extracts text from a .docx file.\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle different file types\n",
    "\n",
    "def read_file(file_path):\n",
    "\n",
    "    if file_path.endswith('.docx'):\n",
    "        return read_docx(file_path)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        return read_txt(file_path)\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        return read_pdf(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .docx, .pdf, or .txt file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts a ZIP file to the specified directory.\n",
    "\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(f\"Extracted files to: {extract_to}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts name and experience from the text.\n",
    "\n",
    "def extract_name_experience(text):\n",
    "    # Updated patterns to look for both \"My name is\" and \"Name:\"\n",
    "    name_match = re.search(r\"(?:My name is|Name:)\\s+([A-Za-z]+\\s[A-Za-z]+)\", text)\n",
    "    name = name_match.group(1) if name_match else \"Name not found\"\n",
    "\n",
    "    # Extract experience\n",
    "    experience_match = re.search(r\"(\\d+)\\s+years of experience\", text)\n",
    "    experience_years = int(experience_match.group(1)) if experience_match else 0\n",
    "\n",
    "    return name, experience_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counts occurrences of each keyword in the text.\n",
    "\n",
    "def count_keywords(text):\n",
    "    keywords = [\"machine learning\", \"data science\", \"python\", \"deep learning\", \"NLP\", \"artificial intelligence\", \"modeling\"]\n",
    "\n",
    "\n",
    "    keyword_count = sum(1 for keyword in keywords if keyword.lower() in text.lower())\n",
    "    return keyword_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the score based on experience and keyword count.\n",
    "\n",
    "def calculate_score(experience_years, keyword_count):\n",
    "    experience_score = 0\n",
    "    if experience_years >= 5:\n",
    "        experience_score = 70  # Full experience score for 5+ years\n",
    "    elif experience_years >= 3:\n",
    "        experience_score = 50  # Medium score for 3-4 years\n",
    "    elif experience_years >= 1:\n",
    "        experience_score = 30  # Base score for 1-2 years\n",
    "    else:\n",
    "        experience_score = 10  # Minimum score for less than 1 year\n",
    "\n",
    "    # Calculate the keyword score (up to 30 points)\n",
    "    keyword_score = min(keyword_count * 5, 30)  # Each keyword up to max 30 points\n",
    "\n",
    "    total_score = experience_score + keyword_score\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts details and scores the resume based on experience and keywords.\n",
    "\n",
    "def screen_resume(text):\n",
    "    name, experience_years = extract_name_experience(text)\n",
    "    keyword_count = count_keywords(text)\n",
    "    score = calculate_score(experience_years, keyword_count)\n",
    "\n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"Experience (years)\": experience_years,\n",
    "        \"Keywords Matched\": keyword_count,\n",
    "        \"Score\": score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts and processes resumes from a ZIP file, specifically focusing on .docx files.\n",
    "#Each resume is read, processed, and scored based on experience and keyword matches.\n",
    "    \n",
    "\n",
    "def process_resumes_from_zip(zip_path):\n",
    "    extract_to = '/content/resumes'  # Temporary directory for extraction\n",
    "    extract_zip(zip_path, extract_to)\n",
    "\n",
    "    results = []\n",
    "    # Walk through the directory to find all .docx files, including in subdirectories\n",
    "    for root, dirs, files in os.walk(extract_to):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith('.docx'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                resume_text = read_docx(file_path)\n",
    "                result = screen_resume(resume_text) #Applying main function\n",
    "                result[\"File\"] = file_name  # Include the file name in the result for reference\n",
    "                results.append(result)\n",
    "                print(f\"Processed file: {file_path}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Skipped non-docx file: {file_name}\")\n",
    "                print()\n",
    "\n",
    "    if not results:\n",
    "        print(\"No valid resumes found in the ZIP file.\")\n",
    "        print()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the ZIP file\n",
    "zip_path = 'Resumes.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files to: /content/resumes\n",
      "Processed file: /content/resumes\\Resumes\\Resume1.docx\n",
      "\n",
      "Processed file: /content/resumes\\Resumes\\Resume2.docx\n",
      "\n",
      "Processed file: /content/resumes\\Resumes\\Resume3.docx\n",
      "\n",
      "Final Results:\n",
      "{'Name': 'Arjun Patel', 'Experience (years)': 6, 'Keywords Matched': 1, 'Score': 75, 'File': 'Resume1.docx'}\n",
      "{'Name': 'Leena Rao', 'Experience (years)': 4, 'Keywords Matched': 1, 'Score': 55, 'File': 'Resume2.docx'}\n",
      "{'Name': 'Nikhil Mehta', 'Experience (years)': 3, 'Keywords Matched': 0, 'Score': 50, 'File': 'Resume3.docx'}\n"
     ]
    }
   ],
   "source": [
    "# Process resumes and output the results\n",
    "results = process_resumes_from_zip(zip_path)\n",
    "if results:\n",
    "    print(\"Final Results:\")\n",
    "    for result in results:\n",
    "        print(result)\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
